{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VqHg98hziFXu",
    "outputId": "cf054c8e-b611-442f-9119-e4aaddc659d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading mnist-dataset, 23112702 bytes compressed\n",
      "[==================================================] 23112702 bytes downloaded\n",
      "Downloaded and uncompressed: mnist-dataset\n",
      "Data source import complete.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
    "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
    "# THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# NOTEBOOK.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tempfile import NamedTemporaryFile\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import unquote, urlparse\n",
    "from urllib.error import HTTPError\n",
    "from zipfile import ZipFile\n",
    "import tarfile\n",
    "import shutil\n",
    "\n",
    "CHUNK_SIZE = 40960\n",
    "DATA_SOURCE_MAPPING = 'mnist-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F102285%2F242592%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240612%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240612T034022Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D08f4ce1d5c0a25526bc1881e687d0c450e32846b3b5f697a16d97fc1711669de6c4254b6d3081bcacca2cef482ebf15fdaac6b7a5643cbba26b070c611d0b454ea4521a1da0aa2fd0d502d8b751b2bb5b5190cab83b016d80e0cc4ded05bafd560a6006a9119ef1e63fe77e365597b07d7da26a67eab2cb45ceadbd94912c1cb36dc6ef7ebbe6f6f9bbcd33a0129a415e88c784577d4efa7d04e9343fdb0f5a7af32cf7a8bede6f54825b14dcf8576069a4e68e59f9184d884feb96552052a30d5c2469f63c8f4eae5dd4e52f9387497a82e3e16d6178d4f6b57806462c9b71bc9e69355f626d19664147ef4fc62e7dd532a771c6bc56ec5e5f0e3f8ef86b145'\n",
    "\n",
    "KAGGLE_INPUT_PATH='/kaggle/input'\n",
    "KAGGLE_WORKING_PATH='/kaggle/working'\n",
    "KAGGLE_SYMLINK='kaggle'\n",
    "\n",
    "!umount /kaggle/input/ 2> /dev/null\n",
    "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
    "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
    "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
    "\n",
    "try:\n",
    "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "try:\n",
    "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
    "except FileExistsError:\n",
    "  pass\n",
    "\n",
    "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
    "    directory, download_url_encoded = data_source_mapping.split(':')\n",
    "    download_url = unquote(download_url_encoded)\n",
    "    filename = urlparse(download_url).path\n",
    "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
    "    try:\n",
    "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
    "            total_length = fileres.headers['content-length']\n",
    "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
    "            dl = 0\n",
    "            data = fileres.read(CHUNK_SIZE)\n",
    "            while len(data) > 0:\n",
    "                dl += len(data)\n",
    "                tfile.write(data)\n",
    "                done = int(50 * dl / int(total_length))\n",
    "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
    "                sys.stdout.flush()\n",
    "                data = fileres.read(CHUNK_SIZE)\n",
    "            if filename.endswith('.zip'):\n",
    "              with ZipFile(tfile) as zfile:\n",
    "                zfile.extractall(destination_path)\n",
    "            else:\n",
    "              with tarfile.open(tfile.name) as tarfile:\n",
    "                tarfile.extractall(destination_path)\n",
    "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
    "    except HTTPError as e:\n",
    "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
    "        continue\n",
    "    except OSError as e:\n",
    "        print(f'Failed to load {download_url} to path {destination_path}')\n",
    "        continue\n",
    "\n",
    "print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-04T09:44:07.185606Z",
     "iopub.status.busy": "2024-06-04T09:44:07.184717Z",
     "iopub.status.idle": "2024-06-04T09:44:08.212662Z",
     "shell.execute_reply": "2024-06-04T09:44:08.211732Z",
     "shell.execute_reply.started": "2024-06-04T09:44:07.185568Z"
    },
    "id": "tuNDD9bFiFXw",
    "outputId": "2782e9e1-e53c-4250-8447-aa57320905d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mnist-dataset/t10k-images.idx3-ubyte\n",
      "/kaggle/input/mnist-dataset/train-labels.idx1-ubyte\n",
      "/kaggle/input/mnist-dataset/t10k-labels.idx1-ubyte\n",
      "/kaggle/input/mnist-dataset/train-images.idx3-ubyte\n",
      "/kaggle/input/mnist-dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte\n",
      "/kaggle/input/mnist-dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\n",
      "/kaggle/input/mnist-dataset/train-images-idx3-ubyte/train-images-idx3-ubyte\n",
      "/kaggle/input/mnist-dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T10:09:39.885435Z",
     "iopub.status.busy": "2024-06-04T10:09:39.885077Z",
     "iopub.status.idle": "2024-06-04T10:09:39.890576Z",
     "shell.execute_reply": "2024-06-04T10:09:39.88967Z",
     "shell.execute_reply.started": "2024-06-04T10:09:39.885406Z"
    },
    "id": "aLGud6F_iFXw"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ignore specific warnings by message\n",
    "warnings.filterwarnings(\"ignore\", message=\".*divide by zero.*\")\n",
    "\n",
    "# Ignore specific warnings by category\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T09:44:08.215008Z",
     "iopub.status.busy": "2024-06-04T09:44:08.214261Z",
     "iopub.status.idle": "2024-06-04T09:44:08.221072Z",
     "shell.execute_reply": "2024-06-04T09:44:08.220031Z",
     "shell.execute_reply.started": "2024-06-04T09:44:08.21498Z"
    },
    "id": "UTwfY4IbiFXw"
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def read_idx(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        zero, data_type, dims= struct.unpack('>HBB', f.read(4))\n",
    "        shape = tuple(struct.unpack('>I', f.read(4))[0] for d in range(dims))\n",
    "\n",
    "        return np.frombuffer(f.read(), dtype=np.uint8).reshape(shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T09:44:08.223063Z",
     "iopub.status.busy": "2024-06-04T09:44:08.222612Z",
     "iopub.status.idle": "2024-06-04T09:44:08.232788Z",
     "shell.execute_reply": "2024-06-04T09:44:08.231859Z",
     "shell.execute_reply.started": "2024-06-04T09:44:08.223014Z"
    },
    "id": "qPS4klnaiFXx"
   },
   "outputs": [],
   "source": [
    "def load_mnist(image_path, label_path):\n",
    "    images = read_idx(image_path)\n",
    "    labels = read_idx(label_path)\n",
    "    return images, labels\n",
    "\n",
    "train_image_path = '/kaggle/input/mnist-dataset/train-images-idx3-ubyte/train-images-idx3-ubyte'\n",
    "train_label_path = '/kaggle/input/mnist-dataset/train-labels-idx1-ubyte/train-labels-idx1-ubyte'\n",
    "test_image_path =  '/kaggle/input/mnist-dataset/t10k-images-idx3-ubyte/t10k-images-idx3-ubyte'\n",
    "test_label_path =  '/kaggle/input/mnist-dataset/t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-04T09:44:08.235585Z",
     "iopub.status.busy": "2024-06-04T09:44:08.234863Z",
     "iopub.status.idle": "2024-06-04T09:44:08.708244Z",
     "shell.execute_reply": "2024-06-04T09:44:08.70727Z",
     "shell.execute_reply.started": "2024-06-04T09:44:08.235553Z"
    },
    "id": "Dal5D7f8iFXx",
    "outputId": "411d69ff-9f7c-45e7-b2b2-5bb35957848c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (60000, 28, 28)\n",
      "Train labels shape: (60000,)\n",
      "Test images shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "train_images, train_labels = load_mnist(train_image_path, train_label_path)\n",
    "test_images, test_labels = load_mnist(test_image_path, test_label_path)\n",
    "print(f'Train images shape: {train_images.shape}')\n",
    "print(f'Train labels shape: {train_labels.shape}')\n",
    "print(f'Test images shape: {test_images.shape}')\n",
    "print(f'Test labels shape: {test_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T09:44:08.709656Z",
     "iopub.status.busy": "2024-06-04T09:44:08.70935Z",
     "iopub.status.idle": "2024-06-04T09:44:08.714544Z",
     "shell.execute_reply": "2024-06-04T09:44:08.713569Z",
     "shell.execute_reply.started": "2024-06-04T09:44:08.709631Z"
    },
    "id": "75P8YNMAiFXx"
   },
   "outputs": [],
   "source": [
    "train_images_flat = train_images.reshape(train_images.shape[0], -1)\n",
    "test_images_flat = test_images.reshape(test_images.shape[0], -1)\n",
    "\n",
    "X_train = train_images_flat\n",
    "y_train = train_labels\n",
    "X_test = test_images_flat\n",
    "y_test = test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-04T09:49:31.851774Z",
     "iopub.status.busy": "2024-06-04T09:49:31.851073Z",
     "iopub.status.idle": "2024-06-04T10:04:29.182567Z",
     "shell.execute_reply": "2024-06-04T10:04:29.181672Z",
     "shell.execute_reply.started": "2024-06-04T09:49:31.851739Z"
    },
    "id": "G6EAOj2TiFXx",
    "outputId": "1c1804da-b165-4242-82a1-1022054edcee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier: 0.9703\n",
      "KNeighborsClassifier: 0.9688\n",
      "SupportVectorMachine: 0.9660\n",
      "MultiLayerPerceptron: 0.9753\n",
      "LogisticRegression: 0.9255\n",
      "XgboostClassifier: 0.9795\n",
      "\n",
      "Accuracies of different classifiers:\n",
      "RandomForestClassifier: 0.9703\n",
      "KNeighborsClassifier: 0.9688\n",
      "SupportVectorMachine: 0.9660\n",
      "MultiLayerPerceptron: 0.9753\n",
      "LogisticRegression: 0.9255\n",
      "XgboostClassifier: 0.9795\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming you have your X_train, y_train, X_test, y_test\n",
    "\n",
    "# Define classifiers\n",
    "classifiers = {\n",
    "    'RandomForestClassifier': RandomForestClassifier(),\n",
    "    'KNeighborsClassifier': KNeighborsClassifier(),\n",
    "    'SupportVectorMachine': make_pipeline(StandardScaler(), SVC()),\n",
    "    'MultiLayerPerceptron': make_pipeline(StandardScaler(), MLPClassifier(max_iter=1000)),\n",
    "    'LogisticRegression': LogisticRegression(),\n",
    "    'XgboostClassifier': XGBClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each classifier\n",
    "accuracies = {}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Predict on the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    # Store accuracy\n",
    "    accuracies[name] = accuracy\n",
    "    print(f\"{name}: {accuracy:.4f}\")\n",
    "\n",
    "# Print all accuracies\n",
    "print(\"\\nAccuracies of different classifiers:\")\n",
    "for name, accuracy in accuracies.items():\n",
    "    print(f\"{name}: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-04T10:09:45.515447Z",
     "iopub.status.busy": "2024-06-04T10:09:45.51509Z",
     "iopub.status.idle": "2024-06-04T10:10:31.958253Z",
     "shell.execute_reply": "2024-06-04T10:10:31.957224Z",
     "shell.execute_reply.started": "2024-06-04T10:09:45.515419Z"
    },
    "id": "SQDAOv0oiFXx",
    "outputId": "d480dc58-6754-4869-d9f1-ac939bdc53b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n",
      "Shape of X_train: (60000, 28, 28, 1)\n",
      "Shape of y_train: (60000, 10)\n",
      "Shape of X_test: (10000, 28, 28, 1)\n",
      "Shape of y_test: (10000, 10)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 13, 13, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 64)          36928     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                36928     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93322 (364.54 KB)\n",
      "Trainable params: 93322 (364.54 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "750/750 [==============================] - 40s 51ms/step - loss: 0.2077 - accuracy: 0.9375 - val_loss: 0.0776 - val_accuracy: 0.9774\n",
      "Epoch 2/20\n",
      "750/750 [==============================] - 38s 51ms/step - loss: 0.0530 - accuracy: 0.9840 - val_loss: 0.0454 - val_accuracy: 0.9869\n",
      "Epoch 3/20\n",
      "750/750 [==============================] - 40s 54ms/step - loss: 0.0377 - accuracy: 0.9877 - val_loss: 0.0494 - val_accuracy: 0.9846\n",
      "Epoch 4/20\n",
      "750/750 [==============================] - 39s 52ms/step - loss: 0.0289 - accuracy: 0.9908 - val_loss: 0.0401 - val_accuracy: 0.9883\n",
      "Epoch 5/20\n",
      "750/750 [==============================] - 36s 49ms/step - loss: 0.0233 - accuracy: 0.9925 - val_loss: 0.0361 - val_accuracy: 0.9894\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - 39s 51ms/step - loss: 0.0179 - accuracy: 0.9941 - val_loss: 0.0444 - val_accuracy: 0.9883\n",
      "Epoch 7/20\n",
      "750/750 [==============================] - 38s 50ms/step - loss: 0.0165 - accuracy: 0.9943 - val_loss: 0.0442 - val_accuracy: 0.9877\n",
      "Epoch 8/20\n",
      "750/750 [==============================] - 40s 54ms/step - loss: 0.0125 - accuracy: 0.9959 - val_loss: 0.0385 - val_accuracy: 0.9892\n",
      "Epoch 9/20\n",
      "750/750 [==============================] - 37s 49ms/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.0378 - val_accuracy: 0.9892\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - 38s 50ms/step - loss: 0.0101 - accuracy: 0.9968 - val_loss: 0.0422 - val_accuracy: 0.9898\n",
      "Epoch 11/20\n",
      "750/750 [==============================] - 40s 53ms/step - loss: 0.0091 - accuracy: 0.9970 - val_loss: 0.0566 - val_accuracy: 0.9879\n",
      "Epoch 12/20\n",
      "750/750 [==============================] - 39s 52ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 0.0499 - val_accuracy: 0.9885\n",
      "Epoch 13/20\n",
      "750/750 [==============================] - 41s 54ms/step - loss: 0.0070 - accuracy: 0.9972 - val_loss: 0.0456 - val_accuracy: 0.9903\n",
      "Epoch 14/20\n",
      "750/750 [==============================] - 36s 48ms/step - loss: 0.0061 - accuracy: 0.9979 - val_loss: 0.0467 - val_accuracy: 0.9896\n",
      "Epoch 15/20\n",
      "750/750 [==============================] - 38s 50ms/step - loss: 0.0075 - accuracy: 0.9972 - val_loss: 0.0469 - val_accuracy: 0.9900\n",
      "Epoch 16/20\n",
      "750/750 [==============================] - 37s 50ms/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.0493 - val_accuracy: 0.9901\n",
      "Epoch 17/20\n",
      "750/750 [==============================] - 36s 48ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.0488 - val_accuracy: 0.9897\n",
      "Epoch 18/20\n",
      "750/750 [==============================] - 38s 50ms/step - loss: 0.0060 - accuracy: 0.9979 - val_loss: 0.0466 - val_accuracy: 0.9904\n",
      "Epoch 19/20\n",
      "750/750 [==============================] - 40s 53ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0523 - val_accuracy: 0.9912\n",
      "Epoch 20/20\n",
      "750/750 [==============================] - 37s 49ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0458 - val_accuracy: 0.9916\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.0331 - accuracy: 0.9931\n",
      "Test accuracy: 0.9931\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the input images to the range [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Reshape the data to include the channel dimension (required for CNNs)\n",
    "# Assuming images are 28x28 pixels\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Verify the shape of the data\n",
    "print(f'Shape of X_train: {X_train.shape}')\n",
    "print(f'Shape of y_train: {y_train.shape}')\n",
    "print(f'Shape of X_test: {X_test.shape}')\n",
    "print(f'Shape of y_test: {y_test.shape}')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential()\n",
    "\n",
    "# First convolutional layer\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Second convolutional layer\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "# Third convolutional layer\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "# Flatten the output from the convolutional layers\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Output layer with softmax activation for classification\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-06-04T10:14:01.014082Z",
     "iopub.status.busy": "2024-06-04T10:14:01.013354Z",
     "iopub.status.idle": "2024-06-04T10:14:01.021899Z",
     "shell.execute_reply": "2024-06-04T10:14:01.020942Z",
     "shell.execute_reply.started": "2024-06-04T10:14:01.014049Z"
    },
    "id": "IkJWG2f2iFXx",
    "outputId": "0915cca7-c8a9-4a4d-ae2b-14d420a9ed48"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+----------------+\n",
      "| Model                  |   Accuracy (%) |\n",
      "+========================+================+\n",
      "| RandomForestClassifier |          97.03 |\n",
      "+------------------------+----------------+\n",
      "| KNeighborsClassifier   |          96.88 |\n",
      "+------------------------+----------------+\n",
      "| SupportVectorMachine   |          96.6  |\n",
      "+------------------------+----------------+\n",
      "| MultiLayerPerceptron   |          97.53 |\n",
      "+------------------------+----------------+\n",
      "| LogisticRegression     |          92.55 |\n",
      "+------------------------+----------------+\n",
      "| XgboostClassifier      |          97.95 |\n",
      "+------------------------+----------------+\n",
      "| CNN                    |          99.31 |\n",
      "+------------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "def create_table(accuracies):\n",
    "  \"\"\"\n",
    "  Creates a table from a dictionary of model names and their accuracies.\n",
    "\n",
    "  Args:\n",
    "    accuracies: A dictionary with model names as keys and accuracies as values.\n",
    "\n",
    "  Returns:\n",
    "    A string containing the formatted table.\n",
    "  \"\"\"\n",
    "\n",
    "  # Multiply all values by 100\n",
    "  accuracies = {name: accuracy * 100 for name, accuracy in accuracies.items()}\n",
    "\n",
    "  # Add CNN and test_acc\n",
    "  accuracies[\"CNN\"] = test_acc * 100\n",
    "\n",
    "  # Create the table\n",
    "  table = tabulate(accuracies.items(), headers=[\"Model\", \"Accuracy (%)\"], tablefmt=\"grid\")\n",
    "\n",
    "  return table\n",
    "\n",
    "\n",
    "\n",
    "table = create_table(accuracies)\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Digit Recognizer Solution - 99 % Accuracy",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 102285,
     "sourceId": 242592,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30716,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
