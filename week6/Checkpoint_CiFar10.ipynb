{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Av568RuMvfjv",
        "outputId": "935b5470-620a-4bc0-a1bc-30df42d9c3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, BatchNormalization, Activation, Dropout, Dense, MaxPooling2D, Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
        "\n",
        "# Normalize the images\n",
        "train_images = train_images.astype('float32') / 255\n",
        "test_images = test_images.astype('float32') / 255\n",
        "\n",
        "# One hot encode the labels\n",
        "num_classes = 10\n",
        "train_labels = keras.utils.to_categorical(train_labels, 10)\n",
        "test_labels = keras.utils.to_categorical(test_labels, 10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "datagen.fit(train_images)"
      ],
      "metadata": {
        "id": "abR4fPMmv5fp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a sequential model and adding layers to it\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(32,32,3)))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(layers.Dropout(0.5))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.BatchNormalization())\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(num_classes, activation='softmax'))    # num_classes = 10\n",
        "\n",
        "# Checking the model summary\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZx5apAdwNJa",
        "outputId": "560f7f67-7a19-47c6-ffda-8e19deb62ed9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 32, 32, 32)        896       \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 32, 32, 32)        128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 32, 32, 32)        128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 16, 16, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_3 (Bat  (None, 16, 16, 64)        256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 8, 8, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 8, 8, 128)         512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 4, 4, 128)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2048)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               262272    \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 128)               512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 552874 (2.11 MB)\n",
            "Trainable params: 551722 (2.10 MB)\n",
            "Non-trainable params: 1152 (4.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "a802t0IXweQL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learning rate scheduler\n",
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 80:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 60:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 40:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 20:\n",
        "        lr *= 1e-1\n",
        "    return lr\n",
        "\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)"
      ],
      "metadata": {
        "id": "Z8efdlgyxJ5_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the checkpoint callback\n",
        "model_checkpoint = ModelCheckpoint(\n",
        "    filepath='/content/{epoch:02d}.ckpt',  # Filepath to save the model\n",
        "    save_freq = \"epoch\",\n",
        "    period = 10,\n",
        "    save_weights_only=True,  # Save only the model weights\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "callbacks_list = [model_checkpoint]\n",
        "# Train the model\n",
        "history = model.fit(datagen.flow(train_images, train_labels, batch_size=64),\n",
        "                    #steps_per_epoch=len(train_images) // 64,\n",
        "                    epochs=100,\n",
        "                    validation_data=(test_images, test_labels),\n",
        "                    callbacks=[lr_scheduler, model_checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MwABm56Ixbh7",
        "outputId": "350ced59-59bd-4020-e3b2-b12f4aab9ef7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "782/782 [==============================] - 55s 51ms/step - loss: 1.8244 - accuracy: 0.3652 - val_loss: 1.4569 - val_accuracy: 0.4886 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 1.3306 - accuracy: 0.5210 - val_loss: 1.4138 - val_accuracy: 0.5450 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 1.1210 - accuracy: 0.6032 - val_loss: 1.0591 - val_accuracy: 0.6449 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 1.0083 - accuracy: 0.6477 - val_loss: 0.9742 - val_accuracy: 0.6584 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.9281 - accuracy: 0.6754 - val_loss: 1.0137 - val_accuracy: 0.6666 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.8777 - accuracy: 0.6964 - val_loss: 0.9274 - val_accuracy: 0.6983 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.8193 - accuracy: 0.7170 - val_loss: 0.6678 - val_accuracy: 0.7734 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.7866 - accuracy: 0.7294 - val_loss: 0.6886 - val_accuracy: 0.7664 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.7583 - accuracy: 0.7387 - val_loss: 0.7758 - val_accuracy: 0.7507 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.7327 - accuracy: 0.7491\n",
            "Epoch 10: saving model to /content/10.ckpt\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.7328 - accuracy: 0.7490 - val_loss: 0.6490 - val_accuracy: 0.7794 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.7134 - accuracy: 0.7548 - val_loss: 0.6997 - val_accuracy: 0.7744 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.6916 - accuracy: 0.7634 - val_loss: 0.8080 - val_accuracy: 0.7389 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.6766 - accuracy: 0.7689 - val_loss: 0.7722 - val_accuracy: 0.7475 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "782/782 [==============================] - 33s 43ms/step - loss: 0.6554 - accuracy: 0.7769 - val_loss: 0.6658 - val_accuracy: 0.7810 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.6463 - accuracy: 0.7789 - val_loss: 0.7008 - val_accuracy: 0.7707 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.6353 - accuracy: 0.7858 - val_loss: 0.5761 - val_accuracy: 0.8067 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "782/782 [==============================] - 33s 43ms/step - loss: 0.6232 - accuracy: 0.7885 - val_loss: 0.6098 - val_accuracy: 0.7970 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.6090 - accuracy: 0.7932 - val_loss: 0.5339 - val_accuracy: 0.8208 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "782/782 [==============================] - 31s 40ms/step - loss: 0.5993 - accuracy: 0.7952 - val_loss: 0.6014 - val_accuracy: 0.8018 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.5940 - accuracy: 0.7968\n",
            "Epoch 20: saving model to /content/20.ckpt\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.5940 - accuracy: 0.7968 - val_loss: 0.5377 - val_accuracy: 0.8166 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "782/782 [==============================] - 33s 43ms/step - loss: 0.5855 - accuracy: 0.7985 - val_loss: 0.5061 - val_accuracy: 0.8287 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.5390 - accuracy: 0.8166 - val_loss: 0.4554 - val_accuracy: 0.8445 - lr: 1.0000e-04\n",
            "Epoch 23/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.5249 - accuracy: 0.8210 - val_loss: 0.4491 - val_accuracy: 0.8453 - lr: 1.0000e-04\n",
            "Epoch 24/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.5238 - accuracy: 0.8209 - val_loss: 0.4531 - val_accuracy: 0.8449 - lr: 1.0000e-04\n",
            "Epoch 25/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.5174 - accuracy: 0.8243 - val_loss: 0.4416 - val_accuracy: 0.8512 - lr: 1.0000e-04\n",
            "Epoch 26/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.5095 - accuracy: 0.8259 - val_loss: 0.4406 - val_accuracy: 0.8512 - lr: 1.0000e-04\n",
            "Epoch 27/100\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.5088 - accuracy: 0.8263 - val_loss: 0.4344 - val_accuracy: 0.8511 - lr: 1.0000e-04\n",
            "Epoch 28/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.5045 - accuracy: 0.8282 - val_loss: 0.4531 - val_accuracy: 0.8493 - lr: 1.0000e-04\n",
            "Epoch 29/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.5006 - accuracy: 0.8284 - val_loss: 0.4391 - val_accuracy: 0.8537 - lr: 1.0000e-04\n",
            "Epoch 30/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.4957 - accuracy: 0.8309\n",
            "Epoch 30: saving model to /content/30.ckpt\n",
            "782/782 [==============================] - 33s 43ms/step - loss: 0.4957 - accuracy: 0.8309 - val_loss: 0.4493 - val_accuracy: 0.8499 - lr: 1.0000e-04\n",
            "Epoch 31/100\n",
            "782/782 [==============================] - 35s 45ms/step - loss: 0.4989 - accuracy: 0.8307 - val_loss: 0.4326 - val_accuracy: 0.8553 - lr: 1.0000e-04\n",
            "Epoch 32/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4921 - accuracy: 0.8328 - val_loss: 0.4367 - val_accuracy: 0.8540 - lr: 1.0000e-04\n",
            "Epoch 33/100\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.4867 - accuracy: 0.8332 - val_loss: 0.4466 - val_accuracy: 0.8513 - lr: 1.0000e-04\n",
            "Epoch 34/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4865 - accuracy: 0.8326 - val_loss: 0.4416 - val_accuracy: 0.8534 - lr: 1.0000e-04\n",
            "Epoch 35/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4900 - accuracy: 0.8321 - val_loss: 0.4443 - val_accuracy: 0.8533 - lr: 1.0000e-04\n",
            "Epoch 36/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4821 - accuracy: 0.8351 - val_loss: 0.4321 - val_accuracy: 0.8562 - lr: 1.0000e-04\n",
            "Epoch 37/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4846 - accuracy: 0.8344 - val_loss: 0.4389 - val_accuracy: 0.8563 - lr: 1.0000e-04\n",
            "Epoch 38/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4808 - accuracy: 0.8368 - val_loss: 0.4533 - val_accuracy: 0.8513 - lr: 1.0000e-04\n",
            "Epoch 39/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4782 - accuracy: 0.8371 - val_loss: 0.4271 - val_accuracy: 0.8573 - lr: 1.0000e-04\n",
            "Epoch 40/100\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.4827 - accuracy: 0.8350\n",
            "Epoch 40: saving model to /content/40.ckpt\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4825 - accuracy: 0.8351 - val_loss: 0.4220 - val_accuracy: 0.8602 - lr: 1.0000e-04\n",
            "Epoch 41/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4750 - accuracy: 0.8378 - val_loss: 0.4179 - val_accuracy: 0.8609 - lr: 1.0000e-04\n",
            "Epoch 42/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4715 - accuracy: 0.8388 - val_loss: 0.4238 - val_accuracy: 0.8596 - lr: 1.0000e-05\n",
            "Epoch 43/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4695 - accuracy: 0.8408 - val_loss: 0.4221 - val_accuracy: 0.8602 - lr: 1.0000e-05\n",
            "Epoch 44/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4716 - accuracy: 0.8396 - val_loss: 0.4187 - val_accuracy: 0.8602 - lr: 1.0000e-05\n",
            "Epoch 45/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4699 - accuracy: 0.8405 - val_loss: 0.4198 - val_accuracy: 0.8602 - lr: 1.0000e-05\n",
            "Epoch 46/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4695 - accuracy: 0.8407 - val_loss: 0.4230 - val_accuracy: 0.8594 - lr: 1.0000e-05\n",
            "Epoch 47/100\n",
            "782/782 [==============================] - 33s 43ms/step - loss: 0.4697 - accuracy: 0.8395 - val_loss: 0.4249 - val_accuracy: 0.8584 - lr: 1.0000e-05\n",
            "Epoch 48/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4676 - accuracy: 0.8425 - val_loss: 0.4236 - val_accuracy: 0.8597 - lr: 1.0000e-05\n",
            "Epoch 49/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4712 - accuracy: 0.8399 - val_loss: 0.4204 - val_accuracy: 0.8611 - lr: 1.0000e-05\n",
            "Epoch 50/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.4705 - accuracy: 0.8388\n",
            "Epoch 50: saving model to /content/50.ckpt\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4705 - accuracy: 0.8388 - val_loss: 0.4202 - val_accuracy: 0.8606 - lr: 1.0000e-05\n",
            "Epoch 51/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4696 - accuracy: 0.8399 - val_loss: 0.4232 - val_accuracy: 0.8591 - lr: 1.0000e-05\n",
            "Epoch 52/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4637 - accuracy: 0.8420 - val_loss: 0.4238 - val_accuracy: 0.8598 - lr: 1.0000e-05\n",
            "Epoch 53/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.4682 - accuracy: 0.8404 - val_loss: 0.4198 - val_accuracy: 0.8603 - lr: 1.0000e-05\n",
            "Epoch 54/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4671 - accuracy: 0.8405 - val_loss: 0.4235 - val_accuracy: 0.8594 - lr: 1.0000e-05\n",
            "Epoch 55/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4691 - accuracy: 0.8398 - val_loss: 0.4217 - val_accuracy: 0.8603 - lr: 1.0000e-05\n",
            "Epoch 56/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4694 - accuracy: 0.8392 - val_loss: 0.4226 - val_accuracy: 0.8613 - lr: 1.0000e-05\n",
            "Epoch 57/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.4680 - accuracy: 0.8393 - val_loss: 0.4243 - val_accuracy: 0.8594 - lr: 1.0000e-05\n",
            "Epoch 58/100\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.4654 - accuracy: 0.8410 - val_loss: 0.4225 - val_accuracy: 0.8602 - lr: 1.0000e-05\n",
            "Epoch 59/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4753 - accuracy: 0.8389 - val_loss: 0.4196 - val_accuracy: 0.8611 - lr: 1.0000e-05\n",
            "Epoch 60/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.4673 - accuracy: 0.8410\n",
            "Epoch 60: saving model to /content/60.ckpt\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4673 - accuracy: 0.8410 - val_loss: 0.4242 - val_accuracy: 0.8587 - lr: 1.0000e-05\n",
            "Epoch 61/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4634 - accuracy: 0.8430 - val_loss: 0.4188 - val_accuracy: 0.8615 - lr: 1.0000e-05\n",
            "Epoch 62/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4732 - accuracy: 0.8382 - val_loss: 0.4194 - val_accuracy: 0.8623 - lr: 1.0000e-06\n",
            "Epoch 63/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4693 - accuracy: 0.8392 - val_loss: 0.4199 - val_accuracy: 0.8609 - lr: 1.0000e-06\n",
            "Epoch 64/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.4678 - accuracy: 0.8422 - val_loss: 0.4214 - val_accuracy: 0.8610 - lr: 1.0000e-06\n",
            "Epoch 65/100\n",
            "782/782 [==============================] - 32s 40ms/step - loss: 0.4668 - accuracy: 0.8414 - val_loss: 0.4191 - val_accuracy: 0.8606 - lr: 1.0000e-06\n",
            "Epoch 66/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4659 - accuracy: 0.8432 - val_loss: 0.4198 - val_accuracy: 0.8601 - lr: 1.0000e-06\n",
            "Epoch 67/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4612 - accuracy: 0.8432 - val_loss: 0.4197 - val_accuracy: 0.8621 - lr: 1.0000e-06\n",
            "Epoch 68/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4652 - accuracy: 0.8412 - val_loss: 0.4202 - val_accuracy: 0.8611 - lr: 1.0000e-06\n",
            "Epoch 69/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4671 - accuracy: 0.8404 - val_loss: 0.4199 - val_accuracy: 0.8606 - lr: 1.0000e-06\n",
            "Epoch 70/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.4621 - accuracy: 0.8417\n",
            "Epoch 70: saving model to /content/70.ckpt\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4621 - accuracy: 0.8417 - val_loss: 0.4181 - val_accuracy: 0.8626 - lr: 1.0000e-06\n",
            "Epoch 71/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4695 - accuracy: 0.8418 - val_loss: 0.4208 - val_accuracy: 0.8616 - lr: 1.0000e-06\n",
            "Epoch 72/100\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.4637 - accuracy: 0.8441 - val_loss: 0.4182 - val_accuracy: 0.8624 - lr: 1.0000e-06\n",
            "Epoch 73/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.4614 - accuracy: 0.8420 - val_loss: 0.4192 - val_accuracy: 0.8605 - lr: 1.0000e-06\n",
            "Epoch 74/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.4635 - accuracy: 0.8407 - val_loss: 0.4176 - val_accuracy: 0.8610 - lr: 1.0000e-06\n",
            "Epoch 75/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4666 - accuracy: 0.8423 - val_loss: 0.4210 - val_accuracy: 0.8613 - lr: 1.0000e-06\n",
            "Epoch 76/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4628 - accuracy: 0.8414 - val_loss: 0.4205 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
            "Epoch 77/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4721 - accuracy: 0.8403 - val_loss: 0.4213 - val_accuracy: 0.8608 - lr: 1.0000e-06\n",
            "Epoch 78/100\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.4672 - accuracy: 0.8406 - val_loss: 0.4211 - val_accuracy: 0.8605 - lr: 1.0000e-06\n",
            "Epoch 79/100\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.4612 - accuracy: 0.8412 - val_loss: 0.4185 - val_accuracy: 0.8616 - lr: 1.0000e-06\n",
            "Epoch 80/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.4647 - accuracy: 0.8420\n",
            "Epoch 80: saving model to /content/80.ckpt\n",
            "782/782 [==============================] - 32s 40ms/step - loss: 0.4647 - accuracy: 0.8420 - val_loss: 0.4197 - val_accuracy: 0.8612 - lr: 1.0000e-06\n",
            "Epoch 81/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4649 - accuracy: 0.8413 - val_loss: 0.4204 - val_accuracy: 0.8612 - lr: 1.0000e-06\n",
            "Epoch 82/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4679 - accuracy: 0.8401 - val_loss: 0.4207 - val_accuracy: 0.8606 - lr: 5.0000e-07\n",
            "Epoch 83/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4645 - accuracy: 0.8390 - val_loss: 0.4178 - val_accuracy: 0.8618 - lr: 5.0000e-07\n",
            "Epoch 84/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4662 - accuracy: 0.8399 - val_loss: 0.4210 - val_accuracy: 0.8611 - lr: 5.0000e-07\n",
            "Epoch 85/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4700 - accuracy: 0.8408 - val_loss: 0.4199 - val_accuracy: 0.8612 - lr: 5.0000e-07\n",
            "Epoch 86/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4654 - accuracy: 0.8413 - val_loss: 0.4199 - val_accuracy: 0.8613 - lr: 5.0000e-07\n",
            "Epoch 87/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4670 - accuracy: 0.8399 - val_loss: 0.4196 - val_accuracy: 0.8618 - lr: 5.0000e-07\n",
            "Epoch 88/100\n",
            "782/782 [==============================] - 33s 43ms/step - loss: 0.4631 - accuracy: 0.8416 - val_loss: 0.4201 - val_accuracy: 0.8622 - lr: 5.0000e-07\n",
            "Epoch 89/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4668 - accuracy: 0.8405 - val_loss: 0.4193 - val_accuracy: 0.8606 - lr: 5.0000e-07\n",
            "Epoch 90/100\n",
            "781/782 [============================>.] - ETA: 0s - loss: 0.4673 - accuracy: 0.8397\n",
            "Epoch 90: saving model to /content/90.ckpt\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4673 - accuracy: 0.8397 - val_loss: 0.4205 - val_accuracy: 0.8609 - lr: 5.0000e-07\n",
            "Epoch 91/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4657 - accuracy: 0.8406 - val_loss: 0.4194 - val_accuracy: 0.8614 - lr: 5.0000e-07\n",
            "Epoch 92/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4653 - accuracy: 0.8423 - val_loss: 0.4182 - val_accuracy: 0.8617 - lr: 5.0000e-07\n",
            "Epoch 93/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4672 - accuracy: 0.8406 - val_loss: 0.4187 - val_accuracy: 0.8618 - lr: 5.0000e-07\n",
            "Epoch 94/100\n",
            "782/782 [==============================] - 33s 42ms/step - loss: 0.4614 - accuracy: 0.8435 - val_loss: 0.4196 - val_accuracy: 0.8612 - lr: 5.0000e-07\n",
            "Epoch 95/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4667 - accuracy: 0.8418 - val_loss: 0.4174 - val_accuracy: 0.8619 - lr: 5.0000e-07\n",
            "Epoch 96/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4613 - accuracy: 0.8428 - val_loss: 0.4191 - val_accuracy: 0.8620 - lr: 5.0000e-07\n",
            "Epoch 97/100\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.4658 - accuracy: 0.8417 - val_loss: 0.4196 - val_accuracy: 0.8616 - lr: 5.0000e-07\n",
            "Epoch 98/100\n",
            "782/782 [==============================] - 35s 44ms/step - loss: 0.4659 - accuracy: 0.8406 - val_loss: 0.4195 - val_accuracy: 0.8615 - lr: 5.0000e-07\n",
            "Epoch 99/100\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4655 - accuracy: 0.8405 - val_loss: 0.4187 - val_accuracy: 0.8617 - lr: 5.0000e-07\n",
            "Epoch 100/100\n",
            "782/782 [==============================] - ETA: 0s - loss: 0.4652 - accuracy: 0.8424\n",
            "Epoch 100: saving model to /content/100.ckpt\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.4652 - accuracy: 0.8424 - val_loss: 0.4249 - val_accuracy: 0.8594 - lr: 5.0000e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "checkpoint = tf.train.Checkpoint(model=model)\n",
        "checkpoint_path = \"/content/01.ckpt\"  # Specify the path to your checkpoint file, without the extension\n"
      ],
      "metadata": {
        "id": "JkSg0IocxlST"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming test_images and test_labels are your test datasets\n",
        "predictions = model.predict(test_images)\n",
        "\n",
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f\"Test accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSrgdGxz2oKM",
        "outputId": "7921771f-ab0a-44e5-def9-dac8701288a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step\n",
            "313/313 [==============================] - 1s 4ms/step - loss: 0.4249 - accuracy: 0.8594\n",
            "Test accuracy: 85.94%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5HlN6zqz2z-b"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}